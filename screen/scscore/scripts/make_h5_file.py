import os, sys
import h5py 
import rdkit.Chem as Chem 
import rdkit.Chem.AllChem as AllChem
import random 
import numpy as np
from multiprocessing import Pool
import cPickle as pickle

'''
This script is used to generate an .h5 file containing all the fingerprints 
needed to train the model, so they do not have to be generated on the fly.

It needs to be called with a command-line argument containing the path to the 
.txt file; in our case, this is the txt file generated by get_reaxys_data.py

This script uses a pool of workers to generate fingerprints to speed the 
process up a little.
'''

path = sys.argv[1]

if not os.path.isfile(path):
    quit('Need to specify a file to read from')

if not os.path.isfile(path + '.pkl'):
    data = []
    with open(path, 'r') as f:
        for line in f:
            rex, n, _id = line.strip("\r\n").split(' ')
            r,p = rex.split('>>')
            if ('.' in p) or (not p):
                continue # do not allow multiple products or none
            n = int(n)
            for r_splt in r.split('.'):
                if r_splt:
                    data.append((_id, n, r_splt, p))
    random.seed(123)
    random.shuffle(data)
    print('Read and shuffled {} total data after splitting'.format(len(data)))
    with open(path + '.pkl', 'w') as f:
        pickle.dump(data, f, -1)
    print('dumped data')
else:
    with open(path + '.pkl', 'r') as f:
        data = pickle.load(f)
        print('read data')

FP_len = 1024
FP_rad = 2
def mol_to_fp(mol, radius=FP_rad, nBits=FP_len):
    if mol is None:
        return np.zeros((nBits,), dtype=np.bool)
    return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits, useChirality=True), dtype=np.bool)

def smi_to_fp(smi, radius=FP_rad, nBits=FP_len):
    if not smi:
        return np.zeros((nBits,), dtype=np.bool)
    return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)

def chunks(l, n):
    """Yield successive n-sized chunks from l."""
    for i in range(0, len(l), n):
        yield l[i:i + n]

if os.path.isfile(path + '.h5'):
	f = h5py.File(path + '.h5', 'r+')
	dset = f['data_fps']
else:
	f = h5py.File(path + '.h5', 'w')
	dset = f.create_dataset('data_fps', (len(data)*2, FP_len), dtype=np.bool)

done = 0
try:

	pool = Pool(8)
	ALREADY_DONE = 0
	for lst in chunks(range(ALREADY_DONE, len(data)), 50000): # 50k chunks
		for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][2] for i in lst], chunksize=500)):
			dset[2*lst[i], :] = fp 
		for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][3] for i in lst], chunksize=500)):
			dset[2*lst[i]+1, :] = fp

		print('latest index done: %i/%i' % (lst[-1]+1, len(data)))
		done += len(lst)
		if done >= 250000:
			f.close()
			f = h5py.File(path + '.h5', 'r+')
			dset = f['data_fps']
			done = 0
			print('Closed and reopened file!')

finally:
	f.close()
	pool.close()
	pool.join()

# ###### FOR 2048 BOOLEAN
# FP_len = 2048
# FP_rad = 2
# def mol_to_fp(mol, radius=FP_rad, nBits=FP_len):
#     if mol is None:
#         return np.zeros((nBits,), dtype=np.bool)
#     return np.array(AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=nBits), dtype=np.bool)

# def smi_to_fp(smi, radius=FP_rad, nBits=FP_len):
#     if not smi:
#         return np.zeros((nBits,), dtype=np.bool)
#     return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)

# def chunks(l, n):
#     """Yield successive n-sized chunks from l."""
#     for i in range(0, len(l), n):
#         yield l[i:i + n]

# if os.path.isfile(path + '.2048_h5'):
#   f = h5py.File(path + '.2048_h5', 'r+')
#   dset = f['data_fps']
# else:
#   f = h5py.File(path + '.2048_h5', 'w')
#   dset = f.create_dataset('data_fps', (len(data)*2, FP_len), dtype=np.bool)

# done = 0
# try:

#   pool = Pool(24)
#   ALREADY_DONE = 0
#   for lst in chunks(range(ALREADY_DONE, len(data)), 50000): # 50k chunks
#       # for i, fp in enumerate(parallel(delayed(smi_to_fp)(data[i][2]) for i in lst)):
#       for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][2] for i in lst], chunksize=500)):
#           dset[2*lst[i], :] = fp 
#       # for i, fp in enumerate(parallel(delayed(smi_to_fp)(data[i][3]) for i in lst)):
#       for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][3] for i in lst], chunksize=500)):
#           dset[2*lst[i]+1, :] = fp

#       print('latest index done: %i/%i' % (lst[-1], len(data)))
#       done += len(lst)
#       if done >= 100000:
#           f.close()
#           f = h5py.File(path + '.2048_h5', 'r+')
#           dset = f['data_fps']
#           done = 0
#           print('Closed and reopened file!')

# finally:
#   f.close()
#   pool.close()
#   pool.join()


# ###### FOR UINT8
# FP_len = 1024
# FP_rad = 2
# convFunc=np.array
# dtype = np.uint8

# def mol_to_fp(mol, radius=FP_rad, nBits=FP_len, convFunc=convFunc):
#     if mol is None:
#         return convFunc((nBits,), dtype=dtype)
#     fp = AllChem.GetMorganFingerprint(mol, radius, useChirality=True) # uitnsparsevect
#     fp_folded = np.zeros((nBits,), dtype=dtype)
#     for k, v in fp.GetNonzeroElements().iteritems():
#         fp_folded[k % nBits] += v 
#     return convFunc(fp_folded)

# def smi_to_fp(smi, radius=FP_rad, nBits=FP_len):
#     if not smi:
#         return np.zeros((nBits,), dtype=dtype)
#     return mol_to_fp(Chem.MolFromSmiles(smi), radius, nBits)

# def chunks(l, n):
#     """Yield successive n-sized chunks from l."""
#     for i in range(0, len(l), n):
#         yield l[i:i + n]

# if os.path.isfile(path + '.1024_uint8_h5'):
#     f = h5py.File(path + '.1024_uint8_h5', 'r+')
#     dset = f['data_fps']
# else:
#     f = h5py.File(path + '.1024_uint8_h5', 'w')
#     dset = f.create_dataset('data_fps', (len(data)*2, FP_len), dtype=dtype)

# done = 0
# try:

#     pool = Pool(24)
#     ALREADY_DONE = 0
#     for lst in chunks(range(ALREADY_DONE, len(data)), 100000): # 100k chunks
#         # for i, fp in enumerate(parallel(delayed(smi_to_fp)(data[i][2]) for i in lst)):
#         for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][2] for i in lst], chunksize=500)):
#             dset[2*lst[i], :] = fp 
#         # for i, fp in enumerate(parallel(delayed(smi_to_fp)(data[i][3]) for i in lst)):
#         for i, fp in enumerate(pool.imap(smi_to_fp, [data[i][3] for i in lst], chunksize=500)):
#             dset[2*lst[i]+1, :] = fp

#         print('latest index done: %i/%i' % (lst[-1], len(data)))
#         done += len(lst)
#         if done >= 100000:
#             f.close()
#             f = h5py.File(path + '.1024_uint8_h5', 'r+')
#             dset = f['data_fps']
#             done = 0
#             print('Closed and reopened file!')

# finally:
#     f.close()
#     pool.close()
#     pool.join()
